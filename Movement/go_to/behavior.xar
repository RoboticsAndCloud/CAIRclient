<?xml version="1.0" encoding="UTF-8" ?><ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3"><Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0"><bitmap>media/images/box/root.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram><Box name="Go to" id="1" localization="8" tooltip="This box contains a basic python script that allows the robot to go to the specified location. The destination place should be already present in the map, and the robot should know where it is with respect to the environment before it can go somewhere." x="239" y="81"><bitmap>media/images/box/box-python-script.png</bitmap><script language="4"><content><![CDATA[import sys
import qi
import numpy as np
import motion
import argparse
from naoqi import ALProxy
import os


class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.memory = ALProxy("ALMemory")
        self.memory.subscribeToEvent("TouchChanged", self.getName(), "onTouched")

        self.motionProxy = ALProxy("ALMotion")
        self.motionProxy.wakeUp()
        # For NAO robot
        self.motionProxy.setMotionConfig([["ENABLE_FOOT_CONTACT_PROTECTION", True]])

        self.sASR = ALProxy("ASR2")
        self.al = ALProxy("ALAutonomousLife")
        self.speech_reco_event = "Audio/RecognizedWords"

        self.animated_speech = ALProxy("ALAnimatedSpeech")
        self.voice_speed = "\\RSPD=85\\"
        self.configuration = {"bodyLanguageMode": "contextual"}

        self.gain = 0.2
        self.isRunning = True

    def setAutonomousAbilities(self, blinking, background, awareness, listening, speaking):
        self.al.setAutonomousAbilityEnabled("AutonomousBlinking", blinking)
        self.al.setAutonomousAbilityEnabled("BackgroundMovement", background)
        self.al.setAutonomousAbilityEnabled("BasicAwareness", awareness)
        self.al.setAutonomousAbilityEnabled("ListeningMovement", listening)
        self.al.setAutonomousAbilityEnabled("SpeakingMovement", speaking)

    # Called everytime a robot sensor is touched: stops the movement and sets isRunning to False
    def onTouched(self, strVarName, value):
        self.logger.info("touched")
        self.motionProxy.stopMove()
        self.isRunning = False

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        #put clean-up code here
        pass

    # This function reads the map.txt file and returns a dictionary with the places as keys and the coordinates as values.
    def get_places_from_map(self):
        # The file map.txt will surely exist because it comes empty with the application
        with open("/data/home/nao/.local/share/PackageManager/apps/movement/map.txt", 'r') as f:
            lines = f.readlines()

        places = {}
        for line in lines:
            if line != '\n':
                place = line.split('_')[0]
                self.logger.info(str(place))
                X = float(line.split('_')[1])
                Y = float(line.split('_')[2])
                Theta = float(line.split('_')[3])
                places[place] = (X, Y, Theta)
        return places


    def onInput_onStart(self):
        try:
            where = str(self.memory.getData("CARESSES/go_to"))
            self.logger.info(where)
            where = where.strip()
            if where == "somewhere":
                raise Exception("No data")
        # If there is no parameter, ask for it
        except:
            self.animated_speech.say(self.voice_speed + "Where should I go?", self.configuration)

            # Start speech recognition
            self.sASR.startReco("English", False, True)
            self.setAutonomousAbilities(True, True, True, True, True)

            start_time_silence = time.time() + 1800

            sentence = ""
            reco_speech = self.memory.getData(self.speech_reco_event)
            self.logger.info(str(reco_speech))
            stop = False

            while True:
                while not reco_speech:
                    if sentence and time.time() - start_time_silence > 0.1:
                        self.sASR.stopReco()
                        self.setAutonomousAbilities(False, True, True, True, True)
                        stop = True
                        break
                    reco_speech = self.memory.getData(self.speech_reco_event)
                    print reco_speech
                sys.stdout.flush()
                if stop:
                    break
                if sentence == "":
                    sep = ""
                else:
                    sep = " "

                if reco_speech:
                    sentence = sentence + sep + reco_speech[0][0]
                start_time_silence = time.time()
                self.memory.insertData(self.speech_reco_event, [])
                reco_speech = self.memory.getData(self.speech_reco_event)

                exit_keywords = ["exit", "quit"]
                if sentence.lower() in exit_keywords:
                    self.onInput_onStop()

            self.logger.info(str(sentence))
            self.memory.insertData(self.speech_reco_event, [])
            self.memory.insertData("CARESSES/go_to", str(sentence))
            where = str(self.memory.getData("CARESSES/go_to"))

        places = self.get_places_from_map()

        if where in places.keys():
            Px_w = places[where][0]
            Py_w = places[where][1]
            Pz_w = 1.0
            Ptheta_w = places[where][2]
            destination = [Px_w, Py_w, Pz_w, Ptheta_w]
            print "Destination:", destination

            # Try to move to that place only if the it is known!
            odometry = self.motionProxy.getRobotPosition(True)
            Px_r = odometry[0]
            Py_r = odometry[1]
            Ptheta_r = odometry[2]
            P_r = [Px_r, Py_r, 1.0]

            try:
                transformation_matrix = self.memory.getData("CARESSES/transformation_matrix")
                theta = self.memory.getData("CARESSES/theta")
                Pr_w = np.dot(transformation_matrix, np.array([P_r]).T)
                Pr_w = [round(Pr_w[0], 2), round(Pr_w[1], 2), 1.0, round(Ptheta_r + theta, 2)]
                self.logger.info("Position of the robot wrt the world:" + str(Pr_w))

                relative_destination = [dest - robot_pos for dest, robot_pos in zip(destination, Pr_w)]
                self.logger.info("Relative destination:" + str(relative_destination))
                # Difference between destination and current position (just X, Y and Z - no theta)
                theta_rotation = Pr_w[3]

                rotation_matrix = np.array([[np.cos(theta_rotation), np.sin(theta_rotation)],
                                            [-np.sin(theta_rotation), np.cos(theta_rotation)]])

                x_y_relative_dest = np.array([relative_destination[0], relative_destination[1]])
                relative_movement = np.dot(rotation_matrix, x_y_relative_dest.T)
                self.logger.info("Relative movement: " + str(relative_movement) + str(theta_rotation))
                self.animated_speech.say(self.voice_speed + "I'm going to " + where + ".", self.configuration)

                ######### CHANGE WITH VELOCITIES ###########
                self.motionProxy.moveTo(round(relative_movement[0], 2), round(relative_movement[1], 2), relative_destination[3])

                while self.isRunning and self.motionProxy.moveIsActive():
                    pass

                odometry = self.motionProxy.getRobotPosition(True)
                Px_r = odometry[0]
                Py_r = odometry[1]
                Ptheta_r = odometry[2]
                P_r = [Px_r, Py_r, 1.0]
                Pr_w = np.dot(transformation_matrix, np.array([P_r]).T)
                Pr_w = [round(Pr_w[0], 2), round(Pr_w[1], 2), 1.0, round(Ptheta_r + theta, 2)]
                self.logger.info("Position of the robot wrt the world:" + str(Pr_w))
                self.memory.removeData("CARESSES/go_to")
                self.onInput_onStop()

            except:
                self.animated_speech.say(self.voice_speed + "I need to know where I am with respect to the environment before I can go somewhere.", self.configuration)
                self.memory.removeData("CARESSES/go_to")
                self.onInput_onStop()

        else:
            self.animated_speech.say(self.voice_speed + "I'm sorry I don't know where " + where + " is.", self.configuration)
            self.memory.removeData("CARESSES/go_to")
            self.onInput_onStop()


    def onInput_onStop(self):
        self.onUnload() #it is recommended to reuse the clean-up as the box is stopped
        self.onStopped() #activate the output of the box]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /></Box><Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" /><Link inputowner="0" indexofinput="4" outputowner="1" indexofoutput="4" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box></ChoregrapheProject>